{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, HubertModel\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. HuBERT 특성 추출기 정의\n",
    "class HuBERTFeatureExtractor:\n",
    "    def __init__(self, model_name=\"facebook/hubert-base-ls960\"):\n",
    "        # HuBERT 모델과 프로세서 초기화\n",
    "        self.processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = HubertModel.from_pretrained(model_name)\n",
    "        self.model.eval()  # 평가 모드로 설정\n",
    "\n",
    "    def load_audio(self, audio_file):\n",
    "        # 오디오 파일 로드\n",
    "        waveform, sample_rate = torchaudio.load(audio_file, format=\"wav\")\n",
    "        \n",
    "        return waveform, sample_rate\n",
    "\n",
    "    def preprocess_audio(self, waveform, sample_rate, target_sample_rate=44100, max_length=10):\n",
    "        # 모노로 변환\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # 샘플링 레이트 변환\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        max_samples = target_sample_rate * max_length\n",
    "        if waveform.size(1) > max_samples:\n",
    "            waveform = waveform[:, :max_samples]\n",
    "            \n",
    "        return waveform\n",
    "\n",
    "    def extract_features(self, audio_file):\n",
    "        # 오디오 로드 및 전처리\n",
    "        waveform, sample_rate = self.load_audio(audio_file)\n",
    "        waveform = self.preprocess_audio(waveform, sample_rate)\n",
    "        \n",
    "        # 입력 차원 확인 및 조정\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        elif waveform.dim() == 2:\n",
    "            if waveform.size(0) > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        elif waveform.dim() == 3:\n",
    "            waveform = waveform.squeeze(0)\n",
    "            if waveform.size(0) > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # 특성 추출\n",
    "        inputs = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = inputs.input_values\n",
    "        \n",
    "        # 불필요한 차원 제거\n",
    "        input_values = input_values.squeeze(1)  # (batch_size=1, sequence_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_values)\n",
    "            \n",
    "        features = outputs.last_hidden_state\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터셋 클래스 정의\n",
    "class EmotionDataset:\n",
    "    def __init__(self, file_paths, feature_extractor):\n",
    "        self.file_paths = file_paths\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.file_paths[idx]\n",
    "        features = self.feature_extractor.extract_features(audio_path)\n",
    "        features = features.squeeze(0)  # (sequence_length, hidden_size)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transformer 모델 정의\n",
    "class EmotionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EmotionTransformer, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=16),\n",
    "            num_layers=8\n",
    "        )\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # Transformer를 통해 특성 추출\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = x[-1]  # Sequence의 평균을 사용\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(dataloader, model, label_encoder):\n",
    "    device = torch.device(\"mps\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in dataloader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # 예측값을 레이블로 변환\n",
    "    decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "    return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features = [item for item in batch]\n",
    "    features = torch.nn.utils.rnn.pad_sequence(features, batch_first=True)  # [batch_size, sequence_length, feature_dim]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMDB API 키\n",
    "TMDB_API_KEY = \"df9a0caaf2a07ee6babd7024a6accaf8\"\n",
    "    \n",
    "EMOTION_TO_GENRE = {\n",
    "    '기쁨': 35,  # Comedy\n",
    "    '슬픔': 18,  # Drama\n",
    "    '분노': 53,  # Thriller\n",
    "    '불안': 27,  # Horror\n",
    "    '상처': 80,  # Crime\n",
    "    '당황': 28,  # Action\n",
    "    '중립': 10751,  # Family\n",
    "}\n",
    "\n",
    "def get_recommendations(emotion, result_num=10, api_key=TMDB_API_KEY):\n",
    "    # 감정 매핑 확인\n",
    "    genre_id = EMOTION_TO_GENRE.get(emotion)\n",
    "    if not genre_id:\n",
    "        return f\"'{emotion}'에 해당하는 추천 장르가 없습니다. 감정을 다시 입력해주세요.\"\n",
    "\n",
    "    # TMDB Discover API 호출\n",
    "    url = f\"https://api.themoviedb.org/3/discover/movie\"\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"with_genres\": genre_id,\n",
    "        \"sort_by\": \"popularity.desc\",  # 인기 순으로 정렬\n",
    "        \"language\": \"ko-KR\",          # 한국어 결과\n",
    "        \"vote_average.gte\": 7.0,      # 평점 7 이상\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return f\"TMDB API 호출 실패: {response.status_code}\"\n",
    "\n",
    "    data = response.json()\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    if not results:\n",
    "        return f\"'{emotion}'에 맞는 추천 콘텐츠를 찾을 수 없습니다.\"\n",
    "\n",
    "    # 추천 콘텐츠 추출\n",
    "    recommendations = []\n",
    "    for movie in results[:result_num]:  # 상위 N개만 추출\n",
    "        recommendations.append({\n",
    "            \"title\": movie.get(\"title\"),\n",
    "            \"overview\": movie.get(\"overview\"),\n",
    "            \"vote_average\": movie.get(\"vote_average\"),\n",
    "            \"release_date\": movie.get(\"release_date\"),\n",
    "        })\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/8jkvbs3n7tlcqj21nw5kbbqm0000gn/T/ipykernel_29094/2282715700.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./model.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmotionTransformer(\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "녹음 시작. 'stop'을 입력하면 녹음을 멈춥니다.\n",
      "녹음 중단 중...\n",
      "'output.wav' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 설정\n",
    "fs = 44100  # 샘플링 레이트\n",
    "output_filename = \"output.wav\"  # 저장할 파일 이름\n",
    "stop_recording = False  # 녹음 중단 플래그\n",
    "seconds = 10 # 최대 녹음 시간(초)\n",
    "\n",
    "\n",
    "def record_audio(): # 마이크로 음성을 녹음하는 함수.\n",
    "    global stop_recording, audio_data\n",
    "    print(\"녹음 시작. 'stop'을 입력하면 녹음을 멈춥니다.\")\n",
    "    audio_data = sd.rec(int(60 * fs), samplerate=fs, channels=1, dtype='int16')  # 최대 60초 녹음\n",
    "    while not stop_recording:\n",
    "        sd.sleep(100)  # 짧은 대기(0.1초)\n",
    "    sd.stop()  # 녹음 중단\n",
    "    print(\"녹음 중단 중...\")\n",
    "\n",
    "\n",
    "def wait_for_stop(): # 사용자가 'stop'을 입력할 때까지 대기.\n",
    "    global stop_recording\n",
    "    while not stop_recording:\n",
    "        command = input(\"입력: \")\n",
    "        if command.strip().lower() == \"stop\":\n",
    "            stop_recording = True\n",
    "\n",
    "\n",
    "# 스레드 생성 및 실행\n",
    "recording_thread = threading.Thread(target=record_audio)\n",
    "input_thread = threading.Thread(target=wait_for_stop)\n",
    "\n",
    "recording_thread.start()\n",
    "input_thread.start()\n",
    "\n",
    "recording_thread.join()\n",
    "input_thread.join()\n",
    "\n",
    "# 녹음 데이터를 파일로 저장\n",
    "write(output_filename, fs, audio_data[:fs * seconds])\n",
    "print(f\"'{output_filename}' 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File: output.wav -> Predicted Emotion: 기쁨\n"
     ]
    }
   ],
   "source": [
    "# test_audio_file = [\"./output.wav\"]\n",
    "\n",
    "test_audio_file = [\"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/1.기쁨/0029_G2A4E1S0C0_KJE/0029_G2A4E1S0C0_KJE_001970.wav\", \"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/2.슬픔/0033_G2A3E2S0C0_KMA/0033_G2A3E2S0C0_KMA_000020.wav\", \"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/3.분노/0018_G2A3E3S0C0_JBR/0018_G2A3E3S0C0_JBR_000019.wav\", \"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/4.불안/0012_G1A2E4S0C0_CHY/0012_G1A2E4S0C0_CHY_000011.wav\", \"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/5.상처/0005_G1A3E5S0C0_LJB/0005_G1A3E5S0C0_LJB_000014.wav\", \"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/6.당황/0020_G2A4E6S0C0_HGW/0020_G2A4E6S0C0_HGW_000009.wav\", \"./dataset/015.감성 및 발화 스타일별 음성합성 데이터/01.데이터/2.Validation/원천데이터/1.감정/7.중립/0044_G2A5E7S0C0_KTH/0044_G2A5E7S0C0_KTH_000012.wav\"]\n",
    "\n",
    "with open(\"./label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Feature extractor 설정\n",
    "feature_extractor = HuBERTFeatureExtractor()\n",
    "\n",
    "# 테스트 데이터셋 및 DataLoader 생성\n",
    "test_dataset = EmotionDataset(test_audio_file, feature_extractor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 감정 예측\n",
    "predicted_emotions = predict_emotion(test_dataloader, model, label_encoder)\n",
    "\n",
    "# 결과 출력\n",
    "for audio_file, emotion in zip(test_audio_file, predicted_emotions):\n",
    "    print(f\"Audio File: {os.path.basename(audio_file)} -> Predicted Emotion: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'기쁨'에 맞는 추천 콘텐츠:\n",
      "\n",
      "1. 제목: 수퍼 소닉 3\n",
      "   개봉일: 2024-12-19\n",
      "   평점: 7.628\n",
      "   줄거리: 너클즈, 테일즈와 함께 평화로운 일상을 보내던 초특급 히어로 소닉. 연구 시설에 50년간 잠들어 있던 사상 최강의 비밀 병기 \"섀도우\"가 탈주하자, 세계 수호 통합 부대(약칭 세.수.통)에 의해 극비 소집된다. 소중한 것을 잃은 분노와 복수심에 불타는 섀도우는 소닉의 초고속 스피드와 너클즈의 최강 펀치를 단 단숨에 제압해버린다. 세상을 지배하려는 닥터 로보트닉과 그의 할아버지 제럴드 박사는 섀도우의 엄청난 힘 카오스 에너지를 이용해 인류를 정복하려고 하는데…\n",
      "\n",
      "2. 제목: 모아나 2\n",
      "   개봉일: 2024-11-21\n",
      "   평점: 7.0\n",
      "   줄거리: 바다를 누볐던 선조들에게서 예기치 못한 부름을 받은 모아나가 마우이와 다시 만나 새로운 선원들과 함께 오랫동안 잊혀진 멀고 위험한 바다 너머로 떠나는 특별한 모험을 담은 이야기\n",
      "\n",
      "3. 제목: 레드 원\n",
      "   개봉일: 2024-10-31\n",
      "   평점: 7.066\n",
      "   줄거리: 크리스마스 D-1, 철통같은 보안을 뚫고 코드명 '레드 원' 산타클로스가 납치되고 크리스마스가 사라질 위기에 처했다! '레드 원'을 찾기 위해 사령관 '칼럼 드리프트'는 산타클로스 따위는 없다고 믿는 현상금 사냥꾼 '잭 오말리'와 협력하기로 한다. 시작부터 삐그덕 거리는 이들 앞에 크리스마스의 존재를 위협하는 위험천만한 적들이 나타나는데…\n"
     ]
    }
   ],
   "source": [
    "recommendations = get_recommendations(emotion, 3)\n",
    "\n",
    "if isinstance(recommendations, str):\n",
    "    print(recommendations)  # 에러 메시지 출력\n",
    "else:\n",
    "    print(f\"'{emotion}'에 맞는 추천 콘텐츠:\")\n",
    "    for idx, movie in enumerate(recommendations, 1):\n",
    "        print(f\"\\n{idx}. 제목: {movie['title']}\")\n",
    "        print(f\"   개봉일: {movie['release_date']}\")\n",
    "        print(f\"   평점: {movie['vote_average']}\")\n",
    "        print(f\"   줄거리: {movie['overview']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice-emotion-recognition_prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
